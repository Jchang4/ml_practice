{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pickle.load(open('../data/fitzgerald/processed-all-books.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_dict['X']\n",
    "Y = data_dict['Y']\n",
    "dataX = data_dict['dataX']\n",
    "dataY = data_dict['dataY']\n",
    "char_to_idx = data_dict['char_to_idx']\n",
    "idx_to_char = data_dict['idx_to_char']\n",
    "\n",
    "# Constants\n",
    "m, n_timesteps, _ = X.shape\n",
    "_, n_chars = Y.shape\n",
    "n_a = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (2029021, 140, 1)\n",
      "Y shape: (2029021, 67)\n",
      "Num. Timesteps: 140\n",
      "Num. Unique Chars: 67\n"
     ]
    }
   ],
   "source": [
    "print('X shape:', X.shape)\n",
    "print('Y shape:', Y.shape)\n",
    "print('Num. Timesteps:', n_timesteps)\n",
    "print('Num. Unique Chars:', n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_timesteps, n_features, n_a):\n",
    "    x0 = Input(shape=(n_timesteps,1), name='input')\n",
    "    \n",
    "    X = LSTM(n_a, return_sequences=True)(x0)\n",
    "    X = LSTM(n_a)(X)\n",
    "    out = Dense(n_features, activation='softmax')(X)\n",
    "    model = Model(x0, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(n_timesteps, n_chars, n_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_initial = np.zeros((n_timesteps, n_a))\n",
    "c_initial = np.zeros((n_timesteps, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1623216 samples, validate on 405805 samples\n",
      "Epoch 1/20\n",
      "1623216/1623216 [==============================] - 360s 222us/step - loss: 2.2718 - val_loss: 2.4016\n",
      "Epoch 2/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2643 - val_loss: 2.3880\n",
      "Epoch 3/20\n",
      "1623216/1623216 [==============================] - 355s 219us/step - loss: 2.2574 - val_loss: 2.3911\n",
      "Epoch 4/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2508 - val_loss: 2.3805\n",
      "Epoch 5/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2444 - val_loss: 2.3779\n",
      "Epoch 6/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2386 - val_loss: 2.3776\n",
      "Epoch 7/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2328 - val_loss: 2.3729\n",
      "Epoch 8/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2271 - val_loss: 2.3610\n",
      "Epoch 9/20\n",
      "1623216/1623216 [==============================] - 356s 219us/step - loss: 2.2220 - val_loss: 2.3551\n",
      "Epoch 10/20\n",
      "1623216/1623216 [==============================] - 356s 220us/step - loss: 2.2172 - val_loss: 2.3530\n",
      "Epoch 11/20\n",
      "1623216/1623216 [==============================] - 357s 220us/step - loss: 2.2120 - val_loss: 2.3559\n",
      "Epoch 12/20\n",
      "1623216/1623216 [==============================] - 357s 220us/step - loss: 2.2073 - val_loss: 2.3482\n",
      "Epoch 13/20\n",
      "1623216/1623216 [==============================] - 357s 220us/step - loss: 2.2027 - val_loss: 2.3391\n",
      "Epoch 14/20\n",
      "1623216/1623216 [==============================] - 357s 220us/step - loss: 2.1980 - val_loss: 2.3402\n",
      "Epoch 15/20\n",
      "1623216/1623216 [==============================] - 357s 220us/step - loss: 2.1939 - val_loss: 2.3366\n",
      "Epoch 16/20\n",
      "1623216/1623216 [==============================] - 357s 220us/step - loss: 2.1898 - val_loss: 2.3276\n",
      "Epoch 17/20\n",
      "1623216/1623216 [==============================] - 358s 220us/step - loss: 2.1854 - val_loss: 2.3363\n",
      "Epoch 18/20\n",
      "1623216/1623216 [==============================] - 358s 220us/step - loss: 2.1814 - val_loss: 2.3194\n",
      "Epoch 19/20\n",
      "1623216/1623216 [==============================] - 358s 221us/step - loss: 2.1770 - val_loss: 2.3180\n",
      "Epoch 20/20\n",
      "1623216/1623216 [==============================] - 358s 221us/step - loss: 2.1735 - val_loss: 2.3164\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, \n",
    "                    validation_split=0.2,\n",
    "                    epochs=20, \n",
    "                    batch_size=1024, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 42\n",
    "model.save('../data/trained-fitzgerald-{}-epochs'.format(num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Text\n",
    "# Given seed sentence - 100 characters - predict next character\n",
    "# For every predicted char, add to predictions = []\n",
    "\n",
    "def reshape_input(original_input):\n",
    "    return np.reshape(original_input, (1, n_timesteps, 1))\n",
    "\n",
    "def get_p_idx(p):\n",
    "    flattened = np.ndarray.flatten(np.array(p))\n",
    "    return np.random.choice([i for i in range(n_chars)], \n",
    "                            p = flattened)\n",
    "\n",
    "def generate_text(seed_input, model, num_chars_to_generate=140):\n",
    "    if len(seed_input) < 100:\n",
    "        raise Exception('Seed_input must be at least 140 characters')\n",
    "    curr_input = seed_input.lower()\n",
    "    curr_input = list(curr_input)\n",
    "    curr_input = curr_input[:n_timesteps] # first 100 chars\n",
    "    \n",
    "    curr_input = [char_to_idx[c] for c in curr_input]\n",
    "    # Normalize\n",
    "    curr_input = np.array(curr_input) / n_chars\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(num_chars_to_generate):\n",
    "        p = model.predict(reshape_input(curr_input))\n",
    "        p_idx = get_p_idx(p)\n",
    "        predictions.append(idx_to_char[p_idx])\n",
    "        curr_input = np.append(curr_input[1:], p_idx / n_chars)\n",
    "    \n",
    "    return ''.join(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_input = \"Then abroad again--to Rome this time, where he dallied with architecture and painting in turn, took up the violin, and wrote some ghastly Italian sonnets, supposedly the ruminations of a thirteenth-century monk on the joys of the contemplative life. It became established among his Harvard intimates that he was in Rome, and those of them who were abroad that year looked him up and discovered with him, on many moonlight excursions, much in the city that was older than the Renaissance or indeed than the republic. Maury Noble, from Philadelphia, for instance, remained two months, and together they realized the peculiar charm of Latin women and had a delightful sense of being very young and free in a civilization that was very old and free. Not a few acquaintances of his grandfather's called on him, and had he so desired he might have been _persona grata_ with the diplomatic set--indeed, he found that his inclinations tended more and more toward conviviality, but that long adolescent aloofness and consequent shyness still dictated to his conduct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' heddny,\"shnukng agoiy and cxer crbissalt soatioing vhar fasosned. \"were ceeu i wo dlmver\\'  she copagoy.\\n\"\"pooeer\\'  buai!ttraknly, in grmuor'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(seed_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
