{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Lambda, Reshape, LSTM, Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text, char_to_idx = pickle.load(open('../data/processed-alice.pickle', 'rb'))\n",
    "X, Y = pickle.load(open('../data/alice-x-y-data.pickle', 'rb'))\n",
    "\n",
    "Y = np.reshape(Y, (X.shape[1], X.shape[0], X.shape[2]))\n",
    "\n",
    "# Global Variables\n",
    "m, Tx, n_values = X.shape\n",
    "n_a = 64 # for LSTM with 64-dimensional hidden states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "First we train the model to get the proper weights.\n",
    "Remember, we must declare the layers in our model as global variables so they\n",
    "can be reused in the Text Generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reused Layer for Text Generation: Reshape, LSTM, Dense\n",
    "reshapor = Reshape((1, n_values))\n",
    "LSTM_cell = LSTM(n_a, return_state=True)\n",
    "densor = Dense(n_values, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_to_talk(Tx, n_a, n_values):\n",
    "    X = Input(shape=(Tx, n_values))\n",
    "    \n",
    "    # Define initial hidden state for LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    for t in range(Tx):\n",
    "        # Get t'th vector\n",
    "        x = Lambda(lambda x: X[:,t,:])(X)\n",
    "        # Reshape x to be (1,Tx)\n",
    "        x = reshapor(x)\n",
    "        # Perform 1 step of LSTM\n",
    "        a, _, c = LSTM_cell(x)\n",
    "        # apply densor to hidden state output of LSTM_cell\n",
    "        out = densor(a)\n",
    "        outputs.append(out)\n",
    "    return Model([X, a0, c0], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn_to_talk(Tx, n_a, n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "973/973 [==============================] - 162s 166ms/step - loss: 448.6111 - dense_1_loss: 3.4800 - dense_1_acc: 0.1624 - dense_1_acc_1: 0.3669 - dense_1_acc_2: 0.4286 - dense_1_acc_3: 0.4275 - dense_1_acc_4: 0.3823 - dense_1_acc_5: 0.1357 - dense_1_acc_6: 0.1316 - dense_1_acc_7: 0.0185 - dense_1_acc_8: 0.1768 - dense_1_acc_9: 0.2497 - dense_1_acc_10: 0.2425 - dense_1_acc_11: 0.3895 - dense_1_acc_12: 0.4306 - dense_1_acc_13: 0.4851 - dense_1_acc_14: 0.2878 - dense_1_acc_15: 0.2271 - dense_1_acc_16: 0.2559 - dense_1_acc_17: 0.2004 - dense_1_acc_18: 0.2765 - dense_1_acc_19: 0.1788 - dense_1_acc_20: 0.2528 - dense_1_acc_21: 0.3679 - dense_1_acc_22: 0.3443 - dense_1_acc_23: 0.0226 - dense_1_acc_24: 0.3083 - dense_1_acc_25: 0.2703 - dense_1_acc_26: 0.2117 - dense_1_acc_27: 0.1768 - dense_1_acc_28: 0.3813 - dense_1_acc_29: 0.4974 - dense_1_acc_30: 0.3145 - dense_1_acc_31: 0.3248 - dense_1_acc_32: 0.1387 - dense_1_acc_33: 0.2210 - dense_1_acc_34: 0.4481 - dense_1_acc_35: 0.2816 - dense_1_acc_36: 0.4347 - dense_1_acc_37: 0.4203 - dense_1_acc_38: 0.3926 - dense_1_acc_39: 0.1696 - dense_1_acc_40: 0.0144 - dense_1_acc_41: 0.2220 - dense_1_acc_42: 0.2025 - dense_1_acc_43: 0.1295 - dense_1_acc_44: 0.3916 - dense_1_acc_45: 0.2343 - dense_1_acc_46: 0.3546 - dense_1_acc_47: 0.2703 - dense_1_acc_48: 0.0699 - dense_1_acc_49: 0.3690 - dense_1_acc_50: 0.1244 - dense_1_acc_51: 0.5190 - dense_1_acc_52: 0.2806 - dense_1_acc_53: 0.1665 - dense_1_acc_54: 0.1747 - dense_1_acc_55: 0.3011 - dense_1_acc_56: 0.2436 - dense_1_acc_57: 0.1398 - dense_1_acc_58: 0.1449 - dense_1_acc_59: 0.6053 - dense_1_acc_60: 0.1480 - dense_1_acc_61: 0.1799 - dense_1_acc_62: 0.1953 - dense_1_acc_63: 0.2960 - dense_1_acc_64: 0.4882 - dense_1_acc_65: 0.4121 - dense_1_acc_66: 0.3484 - dense_1_acc_67: 0.2107 - dense_1_acc_68: 0.5026 - dense_1_acc_69: 0.3494 - dense_1_acc_70: 0.2724 - dense_1_acc_71: 0.3566 - dense_1_acc_72: 0.3844 - dense_1_acc_73: 0.3535 - dense_1_acc_74: 0.2652 - dense_1_acc_75: 0.2734 - dense_1_acc_76: 0.4707 - dense_1_acc_77: 0.2333 - dense_1_acc_78: 0.2395 - dense_1_acc_79: 0.2662 - dense_1_acc_80: 0.3279 - dense_1_acc_81: 0.1881 - dense_1_acc_82: 0.1860 - dense_1_acc_83: 0.1089 - dense_1_acc_84: 0.1346 - dense_1_acc_85: 0.2251 - dense_1_acc_86: 0.4666 - dense_1_acc_87: 0.5118 - dense_1_acc_88: 0.2569 - dense_1_acc_89: 0.1336 - dense_1_acc_90: 0.2641 - dense_1_acc_91: 0.1624 - dense_1_acc_92: 0.1326 - dense_1_acc_93: 0.1614 - dense_1_acc_94: 0.2292 - dense_1_acc_95: 0.1346 - dense_1_acc_96: 0.2898 - dense_1_acc_97: 0.2076 - dense_1_acc_98: 0.2117 - dense_1_acc_99: 0.3474 - dense_1_acc_100: 0.3227 - dense_1_acc_101: 0.1007 - dense_1_acc_102: 0.4296 - dense_1_acc_103: 0.4522 - dense_1_acc_104: 0.2693 - dense_1_acc_105: 0.2179 - dense_1_acc_106: 0.1542 - dense_1_acc_107: 0.5026 - dense_1_acc_108: 0.4532 - dense_1_acc_109: 0.3895 - dense_1_acc_110: 0.2590 - dense_1_acc_111: 0.5221 - dense_1_acc_112: 0.3207 - dense_1_acc_113: 0.2035 - dense_1_acc_114: 0.3114 - dense_1_acc_115: 0.2189 - dense_1_acc_116: 0.4687 - dense_1_acc_117: 0.6691 - dense_1_acc_118: 0.3720 - dense_1_acc_119: 0.2528 - dense_1_acc_120: 0.2312 - dense_1_acc_121: 0.2395 - dense_1_acc_122: 0.4923 - dense_1_acc_123: 0.1100 - dense_1_acc_124: 0.4604 - dense_1_acc_125: 0.3340 - dense_1_acc_126: 0.3433 - dense_1_acc_127: 0.3196 - dense_1_acc_128: 0.6351 - dense_1_acc_129: 0.2970 - dense_1_acc_130: 0.2138 - dense_1_acc_131: 0.2580 - dense_1_acc_132: 0.5478 - dense_1_acc_133: 0.2693 - dense_1_acc_134: 0.4378 - dense_1_acc_135: 0.0462 - dense_1_acc_136: 0.3597 - dense_1_acc_137: 0.4101 - dense_1_acc_138: 0.5231 - dense_1_acc_139: 0.0606                                                                                                                                                                                                                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105183860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X, a0, c0], list(Y), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Trained Weights for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
