{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.applications import vgg19\n",
    "from keras.preprocessing.image import load_img, img_to_array, save_img\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Config Variables / Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. Content Layers: 1\n",
      "Num. Style Layers:   5\n"
     ]
    }
   ],
   "source": [
    "# Config Variables / Constants\n",
    "\n",
    "# Images\n",
    "TARGET_HEIGHT = 512\n",
    "TARGET_WIDTH = 512\n",
    "TARGET_CHANNELS = 3\n",
    "\n",
    "# Image Paths\n",
    "CONTENT_PATH = '../data/seal.jpeg'\n",
    "STYLE_PATH = '../data/fauvism-1.jpg'\n",
    "\n",
    "# VGG 19 Layers\n",
    "CONTENT_LAYERS = ['block5_conv2']\n",
    "STYLE_LAYERS = [('block1_conv1', 0.2),\n",
    "                ('block2_conv1', 0.2),\n",
    "                ('block3_conv1', 0.2),\n",
    "                ('block4_conv1', 0.2),\n",
    "                ('block5_conv1', 0.2)]\n",
    "\n",
    "print('Num. Content Layers:', len(CONTENT_LAYERS))\n",
    "print('Num. Style Layers:  ', len(STYLE_LAYERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Helper Functions\n",
    "def show_img(image, title=''):\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def vgg19_preprocess_img(img):\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def preprocess_img(path_to_img, show = False, title = ''):\n",
    "    img = load_img(path_to_img, target_size=(TARGET_HEIGHT, TARGET_WIDTH))\n",
    "    if show:\n",
    "        show_img(img, title)\n",
    "    img = img_to_array(img)\n",
    "    return vgg19_preprocess_img(img)\n",
    "\n",
    "def deprocess_img(img):\n",
    "    x = img.copy()\n",
    "    \n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, 0)\n",
    "    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
    "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "    if len(x.shape) != 3:\n",
    "        raise ValueError(\"Invalid input to deprocess_img()\")\n",
    "    \n",
    "    # Inverse of preprocessing\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    \n",
    "    # Ensure we're in the range of (0, 255)\n",
    "    x = np.clip(x, 0., 255.).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_noise_image(content_image, noise_ratio = 0.6):\n",
    "    \"\"\"\n",
    "    Generates a noisy image by adding random noise to the content_image\n",
    "    \"\"\"\n",
    "    # Generate a random noise_image\n",
    "    noise_image = np.random.uniform(-20, 20, (TARGET_HEIGHT, TARGET_WIDTH, TARGET_CHANNELS)).astype('float32')\n",
    "    \n",
    "    # Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "    input_image = noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "    \n",
    "    return vgg19_preprocess_img(input_image)\n",
    "\n",
    "def generate_img(content_image, noise_ratio = 0.6, show = False):\n",
    "    \"\"\"\n",
    "    Generates a noisy image by adding random noise to the content_image\n",
    "    \"\"\"\n",
    "    # Generate a random noise_image\n",
    "    img = np.random.uniform(-20, 20, (TARGET_HEIGHT, TARGET_WIDTH, TARGET_CHANNELS)).astype('float32')    \n",
    "    # Set the input_image to be a weighted average of the content_image and a noise_image\n",
    "    img = img * noise_ratio + content_image * (1 - noise_ratio)\n",
    "\n",
    "    if show:\n",
    "        show_img(img, 'Generated Image')\n",
    "        \n",
    "    return vgg19_preprocess_img(img)\n",
    "\n",
    "def save_image(path_to_save, img):\n",
    "    img = deprocess_img(img)\n",
    "    save_img(path_to_save, img, file_format=\"jpg\")\n",
    "    print('Saved image!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Helper Functions\n",
    "def mean_squared_error(a, b):\n",
    "    return tf.reduce_mean(tf.square(a - b))\n",
    "\n",
    "def gram_matrix(a):\n",
    "    return tf.matmul(a, tf.transpose(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Helper Functions\n",
    "def get_model():\n",
    "    \"\"\" Load VGG19 model with the appropriate outputs \"\"\"\n",
    "    model = vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "    model.trainable = False\n",
    "    \n",
    "    content_output = [model.get_layer(layer_name).output for layer_name in CONTENT_LAYERS]\n",
    "    style_output = [model.get_layer(layer_name).output for layer_name,coeff in STYLE_LAYERS]\n",
    "    model_outputs = content_output + style_output\n",
    "    \n",
    "    return Model(inputs=model.inputs, outputs=model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Content, Style, and Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/turtle.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-497f3852993c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Content, Style, and Generated Images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontent_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Content Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstyle_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTYLE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Style Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c3898be8bc4a>\u001b[0m in \u001b[0;36mpreprocess_img\u001b[0;34m(path_to_img, show, title)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTARGET_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTARGET_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml_practice/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    493\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    494\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml_practice/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/turtle.png'"
     ]
    }
   ],
   "source": [
    "# Load Content, Style, and Generated Images\n",
    "content_img = preprocess_img(CONTENT_PATH, show=True, title='Content Image')\n",
    "style_img = preprocess_img(STYLE_PATH, show=True, title='Style Image')\n",
    "generated_img = generate_img(content_img[0], show=True)\n",
    "\n",
    "print('Content Shape:  ', content_img.shape)\n",
    "print('Style Shape:    ', style_img.shape)\n",
    "print('Generated Shape:', generated_img.shape)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x = tf.Variable(content_img)\n",
    "    model = get_model()\n",
    "    out = model(x)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in out:\n",
    "        print(i)\n",
    "    for i in sess.run(out):\n",
    "        print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be = 6.76559\n",
      "[[[[-1.683445    1.8942857   4.189092  ]\n",
      "   [ 1.3846824   3.8925915   2.3524866 ]\n",
      "   [-1.9202449   4.6461368  -1.0375276 ]\n",
      "   [ 4.899456   -7.5360813   3.4091651 ]]\n",
      "\n",
      "  [[-0.24858844 -2.4324749   8.146585  ]\n",
      "   [-1.7646906   2.4694333  -6.5859847 ]\n",
      "   [ 0.7149899   0.66104376  4.199985  ]\n",
      "   [ 2.5062335   4.5016236   1.5636368 ]]\n",
      "\n",
      "  [[ 3.4233422  -8.455175    3.4438267 ]\n",
      "   [ 1.2252892   0.23202246  1.4615505 ]\n",
      "   [ 4.211294   -4.7429867  -2.338422  ]\n",
      "   [ 1.3776655  -3.3965898   0.25865   ]]\n",
      "\n",
      "  [[ 1.2512636   7.4767985   5.1221766 ]\n",
      "   [10.617524   -1.3393097   0.99052405]\n",
      "   [-5.410322   -5.450125   -1.7732203 ]\n",
      "   [-4.097825    4.843958    2.7449982 ]]]] [[[[-0.39043474 -4.965909   -5.387548  ]\n",
      "   [ 4.572505    1.1961036   5.0099816 ]\n",
      "   [ 1.7304354  -0.13603461 -0.7514645 ]\n",
      "   [-3.0110965   1.0130516   7.4561086 ]]\n",
      "\n",
      "  [[ 0.51901615 -0.23328066 -0.8221154 ]\n",
      "   [ 0.69788367  1.5624137   0.11127031]\n",
      "   [ 3.7990131  -0.5115707  -5.364818  ]\n",
      "   [-4.8868036  -1.1914248  -0.12090659]]\n",
      "\n",
      "  [[ 7.0109277  -1.2259245   4.2369    ]\n",
      "   [-5.399742    3.159936    7.259596  ]\n",
      "   [ 1.643039    7.3115473   0.17630118]\n",
      "   [-2.8375332   1.839904   -0.71492875]]\n",
      "\n",
      "  [[ 5.2510543   1.4054474   2.8612938 ]\n",
      "   [ 2.5214956   4.3172836   1.902338  ]\n",
      "   [ 0.14868057 -1.2611487  -0.78171515]\n",
      "   [ 0.53150004 -0.8009285   3.4972606 ]]]]\n",
      "J_content = 9.884598\n"
     ]
    }
   ],
   "source": [
    "def compute_content_cost(a_C, a_G):\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    a_C = tf.reshape(a_C, [n_C, n_H * n_W])\n",
    "    a_G = tf.reshape(a_G, [n_C, n_H * n_W])\n",
    "    \n",
    "    constant = 1/(4 * n_H * n_W * n_C)\n",
    "    return constant * tf.reduce_sum(tf.square(a_C - a_G))\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    a_C = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    a_G = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    J_content = compute_content_cost(a_C, a_G)\n",
    "    print(\"J_content = \" + str(J_content.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J_style_layer = 9.190277\n",
      "Should be     = 9.19028\n"
     ]
    }
   ],
   "source": [
    "def compute_style_cost_one_layer(a_S, a_G):\n",
    "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
    "    \n",
    "    a_S = tf.reshape(tf.transpose(a_S), [n_C, n_H * n_W])\n",
    "    a_G = tf.reshape(tf.transpose(a_G), [n_C, n_H * n_W])\n",
    "        \n",
    "    gram_S = gram_matrix(a_S)\n",
    "    gram_G = gram_matrix(a_G)\n",
    "    \n",
    "    constant = 1/(4 * n_C**2 * (n_H * n_W)**2)\n",
    "    return constant * tf.reduce_sum(tf.square(gram_S - gram_G))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    a_S = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    a_G = tf.random_normal([1, 4, 4, 3], mean=1, stddev=4)\n",
    "    J_style_layer = compute_style_cost_one_layer(a_S, a_G)\n",
    "    \n",
    "    print(\"J_style_layer = \" + str(J_style_layer.eval()))\n",
    "    print(\"Should be     =\", str(9.19028))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_style_cost(style_activations, generated_activations):\n",
    "    J_style = 0\n",
    "    for i,(layer_name, coeff) in enumerate(STYLE_LAYERS):\n",
    "        a_S = style_activations[i]\n",
    "        a_G = generated_activations[i]\n",
    "        J_curr = compute_style_cost_one_layer(a_S, a_G)\n",
    "        J_style += coeff * J_curr\n",
    "    return J_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J         = 35.34667875478276\n",
      "Should be = 35.34667875478276\n"
     ]
    }
   ],
   "source": [
    "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
    "    return alpha * J_content + beta * J_style\n",
    "\n",
    "def compute_grads(loss, image):\n",
    "    return K.gradients(loss, image)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as test:\n",
    "    np.random.seed(3)\n",
    "    J_content = np.random.randn()    \n",
    "    J_style = np.random.randn()\n",
    "    J = total_cost(J_content, J_style)\n",
    "    print(\"J         = \" + str(J))\n",
    "    print(\"Should be =\", str(35.34667875478276))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Start interactive session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Images (again)\n",
    "content_img = preprocess_img(CONTENT_PATH, show=False, title='Content Image')\n",
    "style_img = preprocess_img(STYLE_PATH, show=False, title='Style Image')\n",
    "generated_img = generate_img(content_img[0], show=False)\n",
    "# generated_img = tf.Variable(preprocess_img('../images/nst-180.jpg', show=False, title='Generated Image'), name='generated_img', dtype=tf.float32)\n",
    "\n",
    "content_img = tf.constant(content_img, name='content_img', dtype=tf.float32)\n",
    "style_img = tf.constant(style_img, name='style_img', dtype=tf.float32)\n",
    "generated_img = tf.Variable(generated_img, name='generated_img', dtype=tf.float32)\n",
    "\n",
    "# Get the model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_nn(sess, input_image, num_iterations = 500):    \n",
    "    generated_img = input_image\n",
    "    a_C = model(content_img)[0]\n",
    "    a_S = model(style_img)[1:]\n",
    "    a_G = model(generated_img)\n",
    "    \n",
    "    J_content = compute_content_cost(a_C, a_G[0])\n",
    "    J_style = compute_style_cost(a_S, a_G[1:])\n",
    "    J = total_cost(J_content, J_style)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(2.0)\n",
    "    grads = compute_grads(J, generated_img)[0]\n",
    "    train_step = optimizer.apply_gradients([(grads, generated_img)])\n",
    "    \n",
    "    norm_means = np.array([103.939, 116.779, 123.68])\n",
    "    min_vals = -norm_means\n",
    "    max_vals = 255 - norm_means\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_iterations):        \n",
    "        sess.run(train_step)\n",
    "        \n",
    "        clipped = tf.clip_by_value(generated_img, min_vals, max_vals)\n",
    "        generated_img.assign(clipped)\n",
    "        \n",
    "        # Print every 20 iteration.\n",
    "        if i % 20 == 0:\n",
    "            Jt, Jc, Js = sess.run([J, J_content, J_style])\n",
    "            print(\"Iteration \" + str(i) + \" :\")\n",
    "            print(\"total cost = \" + str(Jt))\n",
    "            print(\"content cost = \" + str(Jc))\n",
    "            print(\"style cost = \" + str(Js) + '\\n')\n",
    "            save_img('../images/nst-{}.jpg'.format(i), sess.run(generated_img)[0])\n",
    "    \n",
    "    save_img('../images/nst-final.jpg', sess.run(generated_img)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 :\n",
      "total cost = 202867.8\n",
      "content cost = 0.0007395856\n",
      "style cost = 5071.695\n",
      "\n",
      "Iteration 20 :\n",
      "total cost = 52670.13\n",
      "content cost = 0.00058629934\n",
      "style cost = 1316.753\n",
      "\n",
      "Iteration 40 :\n",
      "total cost = 19341.588\n",
      "content cost = 0.00070345995\n",
      "style cost = 483.53952\n",
      "\n",
      "Iteration 60 :\n",
      "total cost = 11725.771\n",
      "content cost = 0.0008303148\n",
      "style cost = 293.14407\n",
      "\n",
      "Iteration 80 :\n",
      "total cost = 9140.833\n",
      "content cost = 0.0009141279\n",
      "style cost = 228.52061\n",
      "\n",
      "Iteration 100 :\n",
      "total cost = 7684.8027\n",
      "content cost = 0.0009618449\n",
      "style cost = 192.11983\n",
      "\n",
      "Iteration 120 :\n",
      "total cost = 6284.1904\n",
      "content cost = 0.0009896084\n",
      "style cost = 157.10452\n",
      "\n",
      "Iteration 140 :\n",
      "total cost = 4533.4517\n",
      "content cost = 0.0010384838\n",
      "style cost = 113.33603\n",
      "\n",
      "Iteration 160 :\n",
      "total cost = 2846.639\n",
      "content cost = 0.0011566569\n",
      "style cost = 71.16569\n",
      "\n",
      "Iteration 180 :\n",
      "total cost = 1845.3379\n",
      "content cost = 0.0013194766\n",
      "style cost = 46.133118\n",
      "\n",
      "Iteration 200 :\n",
      "total cost = 1231.4457\n",
      "content cost = 0.0014733234\n",
      "style cost = 30.785774\n",
      "\n",
      "Iteration 220 :\n",
      "total cost = 818.2719\n",
      "content cost = 0.0016219387\n",
      "style cost = 20.456392\n",
      "\n",
      "Iteration 240 :\n",
      "total cost = 563.0601\n",
      "content cost = 0.0017558637\n",
      "style cost = 14.076063\n",
      "\n",
      "Iteration 260 :\n",
      "total cost = 420.0411\n",
      "content cost = 0.0018505265\n",
      "style cost = 10.500566\n",
      "\n",
      "Iteration 280 :\n",
      "total cost = 343.69278\n",
      "content cost = 0.0019021288\n",
      "style cost = 8.591845\n",
      "\n",
      "Iteration 300 :\n",
      "total cost = 300.363\n",
      "content cost = 0.0019234484\n",
      "style cost = 7.5085945\n",
      "\n",
      "Iteration 320 :\n",
      "total cost = 271.43042\n",
      "content cost = 0.0019301319\n",
      "style cost = 6.7852783\n",
      "\n",
      "Iteration 340 :\n",
      "total cost = 248.91322\n",
      "content cost = 0.001933518\n",
      "style cost = 6.2223473\n",
      "\n",
      "Iteration 360 :\n",
      "total cost = 229.5311\n",
      "content cost = 0.0019386818\n",
      "style cost = 5.7377925\n",
      "\n",
      "Iteration 380 :\n",
      "total cost = 211.66022\n",
      "content cost = 0.0019470004\n",
      "style cost = 5.2910185\n",
      "\n",
      "Iteration 400 :\n",
      "total cost = 194.17397\n",
      "content cost = 0.0019582785\n",
      "style cost = 4.85386\n",
      "\n",
      "Iteration 420 :\n",
      "total cost = 176.37866\n",
      "content cost = 0.0019718176\n",
      "style cost = 4.4089737\n",
      "\n",
      "Iteration 440 :\n",
      "total cost = 158.06245\n",
      "content cost = 0.0019861567\n",
      "style cost = 3.9510646\n",
      "\n",
      "Iteration 460 :\n",
      "total cost = 139.59381\n",
      "content cost = 0.0020010867\n",
      "style cost = 3.4893453\n",
      "\n",
      "Iteration 480 :\n",
      "total cost = 121.80622\n",
      "content cost = 0.0020148999\n",
      "style cost = 3.0446517\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_nn(sess, generated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
